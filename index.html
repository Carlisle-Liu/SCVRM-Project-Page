<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script type="text/javascript" charset="utf-8"
    src="https://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">

<link href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism.css" rel="stylesheet" />
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-python.min.js"></script>

<style type="text/css">
    body {
        font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight: 300;
        font-size: 17px;
        margin-left: auto;
        margin-right: auto;
    }

    @media screen and (min-width: 980px) {
        body {
            width: 980px;
        }
    }

    h1 {
        font-weight: 300;
        line-height: 1.15em;
    }

    h2 {
        font-size: 1.75em;
    }

    a:link,
    a:visited {
        color: #0E710E;
        text-decoration: none;
    }

    a:hover {
        color: #208799;
    }

    h1 {
        text-align: center;
    }

    h2,
    h3 {
        text-align: left;
    }

    h1 {
        font-size: 40px;
        font-weight: 500;
    }

    h2 {
        font-weight: 400;
        margin: 16px 0px 4px 0px;
    }

    h3 {
        font-weight: 600;
        margin: 16px 0px 4px 0px;
    }

    .paper-title {
        padding: 1px 0px 1px 0px;
    }

    section {
        margin: 32px 0px 32px 0px;
        text-align: justify;
        clear: both;
    }

    .col-5 {
        width: 20%;
        float: left;
    }

    .col-4 {
        width: 25%;
        float: left;
    }

    .col-3 {
        width: 33%;
        float: left;
    }

    .col-2 {
        width: 50%;
        float: left;
    }

    .col-1 {
        width: 100%;
        float: left;
    }

    .author-row,
    .affil-row {
        font-size: 26px;
    }

    .author-row-new {
        text-align: center;
    }

    .author-row-new a {
        display: inline-block;
        font-size: 20px;
        padding: 4px;
    }

    .author-row-new sup {
        color: #313436;
        font-size: 12px;
    }

    .affiliations-new {
        font-size: 18px;
        text-align: center;
        width: 80%;
        margin: 0 auto;
        margin-bottom: 20px;
    }

    .row {
        margin: 16px 0px 16px 0px;
    }

    .authors {
        font-size: 26px;
    }

    .affiliatons {
        font-size: 18px;
    }

    .affil-row {
        margin-top: 18px;
    }

    .teaser {
        max-width: 100%;
    }

    .text-center {
        text-align: center;
    }

    .screenshot {
        width: 256px;
        border: 1px solid #ddd;
    }

    .screenshot-el {
        margin-bottom: 16px;
    }

    hr {
        height: 1px;
        border: 0;
        border-top: 1px solid #ddd;
        margin: 0;
    }

    .material-icons {
        vertical-align: -6px;
    }

    p {
        line-height: 1.25em;
    }

    .caption {
        font-size: 16px;
        color: #666;
        margin-top: 4px;
        margin-bottom: 10px;
    }

    video {
        display: block;
        margin: auto;
    }

    figure {
        display: block;
        margin: auto;
        margin-top: 10px;
        margin-bottom: 10px;
    }

    #bibtex pre {
        font-size: 14px;
        background-color: #eee;
        padding: 16px;
    }

    .green {
        color: #0E710E;
        font-weight: bold;
    }

    .orange {
        color: #d35400;
        font-weight: bold;
    }

    .flex-row {
        display: flex;
        flex-flow: row wrap;
        padding: 0;
        margin: 0;
        list-style: none;
    }

    .coming-soon {
        position: absolute;
        top: -15px;
        right: -15px;
    }

    .paper-btn:hover {
        color: #FF8563;
        transform: translateY(-2px);
    }

    .paper-btn-parent {
        display: flex;
        justify-content: center;
        margin: 16px 0px;
        gap: 25px;
        font-size: 20px;
    }

    .github-btn:hover {
        color: #FF8563;
        transform: translateY(-2px);
    }

    .hf-btn:hover {
        color: #FF8563;
        transform: translateY(-2px);
    }

    .container {
        margin-left: auto;
        margin-right: auto;
        padding-left: 16px;
        padding-right: 16px;
    }

    .venue {
        font-size: 23px;
    }

    .topnav {
        background-color: #EEEEEE;
        overflow: hidden;
    }

    .topnav div {
        max-width: 1070px;
        margin: 0 auto;
    }

    .topnav a {
        display: inline-block;
        color: black;
        text-align: center;
        vertical-align: middle;
        padding: 16px 16px;
        text-decoration: none;
        font-size: 18px;
    }

    .topnav img {
        padding: 2px 0px;
        width: 100%;
        margin: 0.2em 0px 0.3em 0px;
        vertical-align: middle;
    }

    pre {
        font-size: 0.9em;
        padding-left: 7px;
        padding-right: 7px;
        padding-top: 3px;
        padding-bottom: 3px;
        border-radius: 3px;
        background-color: rgb(235, 235, 235);
        overflow-x: auto;
    }

    .download-thumb {
        display: flex;
    }

    @media only screen and (max-width: 620px) {
        .download-thumb {
            display: none;
        }
    }

    .paper-stuff {
        width: 50%;
        font-size: 20px;
    }

    @media only screen and (max-width: 620px) {
        .paper-stuff {
            width: 100%;
        }
    }

    * {
        box-sizing: border-box;
    }

    .column {
        text-align: center;
        float: left;
        width: 16.666%;
        padding: 5px;
    }

    .column3 {
        text-align: center;
        float: left;
        width: 33.333%;
        padding: 5px;
    }

    .border-right {
        border-right: 1px solid black;
    }

    .border-bottom {
        border-bottom: 1px solid black;
    }

    /* Clearfix (clear floats) */
    .row::after {
        content: "";
        clear: both;
        display: table;
    }

    .image-grid {
        display: grid;
        grid-template-columns: 1fr 1.25fr;
        gap: 10px;
        width: 100%;
    }

    .image-grid img {
        width: 100%;
    }

    .image-grid img:first-child {
        object-fit: contain;
    }

    /* Responsive layout - makes the three columns stack on top of each other instead of next to each other */
    @media screen and (max-width: 500px) {
        .column {
            width: 100%;
        }
    }

    @media screen and (max-width: 500px) {
        .column3 {
            width: 100%;
        }
    }

    .figure img {
        width: 100%;
    }

    blockquote {
        background-color: rgba(40, 40, 40, 0.2);
        padding: 0px;
        margin: 2px 0;
        border-radius: 10px;
        padding-top: 1px;
        padding-bottom: 1px;
        padding-left: 5px;
        padding-right: 5px;
    }

    /* Button styles */
    .publication-links {
        display: flex;
        justify-content: center;
        flex-wrap: wrap;
        gap: 10px;
        margin: 15px 0;
    }

    .link-block {
        display: inline-block;
        margin: 0 5px;
    }

    .button.is-dark {
        background-color: #363636;
        color: white;
        padding: 10px 15px;
        font-size: 16px;
        display: inline-flex;
        align-items: center;
        transition: background-color 0.3s, transform 0.2s;
        text-decoration: none;
        border: none;
        cursor: pointer;
    }

    .button.is-dark:hover {
        background-color: #292929;
        transform: translateY(-2px);
    }

    .icon {
        margin-right: 8px;
        display: inline-flex;
        align-items: center;
        justify-content: center;
    }

    .button.is-rounded {
        background-color: #0E4E0E;
        color: white;
        padding: 10px 25px;
        font-size: 16px;
        display: inline-flex;
        align-items: center;
        transition: background-color 0.3s, transform 0.2s;
        text-decoration: none;
        border: none;
        cursor: pointer;
        border-radius: 25px;
    }

    .button.is-rounded:hover {
        background-color: #0A3D0A;
        transform: translateY(-2px);
    }
</style>

<script type="text/javascript" src="../js/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic'
    rel='stylesheet' type='text/css'>

<head>
    <title>Self-Calibrating Vicinal Risk Minimisation</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta property="og:description" content="Self-Calibrating Vicinal Risk Minimisation" />
    <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
    <link href="https://fonts.googleapis.com/css2?family=FontAwesome" rel="stylesheet">
</head>

<body>
    <div class="container">
        <div class="paper-title">
            <h1>
                <span class="green">SCVRM</span>: Self-Calibrating Vicinal Risk Minimisation
            </h1>
        </div>

        <div align="center">
            <a href="https://scholar.google.com.au/citations?user=gqnX0nUAAAAJ&hl=en"
                target="_blank">Jiawei&nbsp;Liu</a><sup>1*</sup> &ensp; <b>&middot;</b> &ensp;
            <a href="https://scholar.google.com.au/citations?user=PkzMdOYAAAAJ&hl=en"
                target="_blank">Changkun&nbsp;Ye</a><sup>1</sup> &ensp; <b>&middot;</b> &ensp;
            <a href="https://scholar.google.com.au/citations?user=yhscu8IAAAAJ&hl=en"
                target="_blank">Ruikai&nbsp;Cui</a><sup>1</sup> &ensp; <b>&middot;</b> &ensp;
            <a href="https://scholar.google.com.au/citations?user=yMXs1WcAAAAJ&hl=en"
                target="_blank">Nick&nbsp;Barnes</a><sup>1</sup>&ensp; <b>&middot;</b> &ensp;
            <br>
            <sup>1</sup> Australian National University
            <br>
            <sup>*</sup>Project Lead<br>
        </div>
        </center>

        <div style="clear: both">
            <div class="publication-links" style="margin-bottom: 30px;">
                <span class="link-block">
                    <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Self-Calibrating_Vicinal_Risk_Minimisation_for_Model_Calibration_CVPR_2024_paper.html"
                        class="external-link button is-rounded" style="background-color: #0E4E0E; color: white;">
                        <span class="icon">
                            <i class="fas fa-file-alt"></i>
                        </span>
                        <span>Paper</span>
                    </a>
                </span>
                <span class="link-block">
                    <a href="https://github.com/Carlisle-Liu/SCVRM" class="external-link button is-rounded"
                        style="background-color: #0E4E0E; color: white;">
                        <span class="icon">
                            <i class="fab fa-github"></i>
                        </span>
                        <span>Code</span>
                    </a>
                </span>
            </div>
        </div>
        <center>
            <div class="figure">
                <img src="assets/SCVRM_Intro.webp"
                    alt="Comparison between SCVRM and Mixup on a 2-D plot and example dense classification data."
                    style="width: 100%;" loading="lazy">
                <!-- <figcaption>
                    An illustration of Mixup and SCVRM on a 2-D plot and example dense classification data where circles
                    with a solid boundary denote labelled images. In Mixup, vicinal images (circles with a dashed
                    boundary) are distributed only on the convex hull of the available data. In contrast, vicinal data
                    in SCVRM can follow an arbitrary vicinal distribution, such as a mixture distribution in a ball as
                    shown here. Furthermore, the vicinal label in SCVRM adopts the ground-truth label (different labels
                    distinguished in colours) of the closest labelled image, but with label confidence (shown in colour
                    intensity) decreased monotonically with increasing Euclidean distance between the vicinal and the
                    labelled images. As a result, label of the vicinal image (crossed circle with olive green colour)
                    has consistently reduced label confidence across all pixels in SCVRM, while presenting irregular
                    patterns in Mixup.
                </figcaption> -->
            </div>
        </center>
    </div>
    <section id="news">
        <h2>News</h2>
        <hr>
        <div class="row">
            <div><span class="material-icons"> event </span> [Sep 2024] Our paper, code, and pretrained models are now
                available on <a href="https://github.com/Carlisle-Liu/SCVRM">GitHub</a>.</div>
        </div>
    </section>

    <section>
        <h2>Overview</h2>
        <hr>
        <p>
            This work addresses a fundamental machine/deep learning problem: <b><em style="color: green;">model
                    calibration - does the empirical class distribution conditioned on probabilistic model prediction
                    matches the predicted distribution.</em></b>
            We propose a <b>Self-Calibrating Vicinal Risk Minimisation (SCVRM)</b> that explores the vicinal data space
            where augmented data with increasing distance to the labelled data are assigned less confident labels.
            Experimental results demonstrate that SCVRM can significantly enhance model calibration for different dense
            classification tasks on both in-distribution and out-of-distribution data.
        </p>

        <!-- <center>
            <div class="figure">
                <img src="assets/a-overview-v4.webp" alt="REPA-E Overview" style="max-width: 100%; height: auto;"
                    loading="lazy">
            </div>
        </center>

        Through extensive evaluations, we demonstrate that our end-to-end training approach <b>REPA-E</b> offers four
        key advantages:
        <p>
            <b>1. Accelerated Generation Performance:</b> REPA-E significantly speeds up diffusion training by over
            <b>17×</b> and <b>45×</b> compared to REPA and vanilla training recipes, respectively, while achieving
            superior quality.
        </p>

        <p>
            <b>2. Improved VAE Latent-Space Structure:</b> Joint tuning adaptively enhances latent space structure
            across different VAE architectures, addressing their specific limitations without explicit regularization.
        </p>

        <p>
            <b>3. Superior Drop-in VAE Replacements:</b> The resulting <b>E2E-VAE</b> serves as a <b>drop-in
                replacement</b> for existing VAEs (e.g., SD-VAE), improving convergence and generation quality across
            diverse LDM architectures.
        </p>

        <p>
            <b>4. Effective From-Scratch Training:</b> REPA-E enables joint training of both VAE and LDM from scratch,
            still achieving superior performance compared to traditional training approaches.
        </p> -->
    </section>

    <section id="1 - code snippet">
        <h2>1. Using SCVRM in your code base</h2>
        <p>Define the vicinal sampling function as in the paper:</p>
        <pre><code class="language-python">
        def scvrm_data(x, y, eta, zeta):
            """Returns vicinal data (input-label pair)"""
            b = x.shape[0]
            sqrt_d = math.sqrt(x[0, :, :, :].numel())
        
            # Vicinal inputs according to Equation (13).
            z = torch.rand_like(x).cuda()
            v = (torch.rand(b).cuda() * zeta + 1.0) * sqrt_d
            z_norm = z.view(b, -1).norm(p = 2, dim = 1)
            add_noise = v.view(b, 1, 1, 1) * z / z_norm.view(b, 1, 1, 1)
            x_vicinal = x.clone().detach() + add_noise
        
            # Vicinal labels according to Equations (8) and (9).
            add_noise_norm = add_noise.view(b, -1).norm(p = 2, dim = 1)
            varphi = 1 - torch.exp(- (add_noise_norm / (eta * sqrt_d)).pow(2))
        
            y_vicinal = (1 - varphi).view(b, 1) * y + (varphi / 10).view(b, 1)
        
            return x_vicinal, y_vicinal
        </code></pre>
    </section>

    <section id="2-improved-latent-space">
        <h2>2. End-to-End Training Improves VAE Latent-Space Structure</h2>
        <hr>
        <ul>
            <li><b>Adaptive refinement without explicit regularization:</b> REPA-E automatically adapts to each VAE's
                unique latent space characteristics without requiring manual heuristic-based regularization</li>
            <li><b>PCA visualization:</b> Using principal component analysis, we project VAE latents to RGB channels,
                revealing how end-to-end tuning with REPA-E improves latent representation quality</li>
            <li><b>Architecture-specific benefits:</b></li>
            <ul>
                <li><b>SD-VAE enhancement:</b> Reduces high-frequency noise components for smoother latent
                    representations</li>
                <li><b>IN-VAE & VA-VAE enhancement:</b> Adds essential structural details to over-smoothed latent
                    representations</li>
            </ul>
        </ul>

        <center>
            <div class="figure">
                <img src="assets/pca-analysis-v11.webp" alt="PCA Analysis of Latent Space"
                    style="max-width: 90%; height: auto;" loading="lazy">
            </div>
        </center>
    </section>

    <section id="3-superior-drop-in-replacements">
        <h2>3. End-to-End Tuned VAEs as Superior Drop-in Replacements</h2>
        <hr>
        <ul>
            <li><b>Universal improvement:</b> E2E-VAE serves as a drop-in replacement for original VAEs, delivering
                superior performance across diverse diffusion architectures</li>
            <li><b>State-of-the-art generation quality:</b> Achieves gFID of <b>1.26</b> (w/ CFG) and <b>1.83</b> (w/o
                CFG) when training with REPA for 800 epochs</li>
            <li><b>Comprehensive performance superiority:</b> Achieves gFID of <b>3.46</b> with SiT-XL and REPA (vs.
                <b>4.88</b> with VA-VAE and <b>7.90</b> with SD-VAE) and <b>4.20</b> with DiT-XL (vs. <b>4.71</b> with
                VA-VAE and <b>12.29</b> with SD-VAE)
            </li>
            <li><b>Architecture-robust performance:</b> E2E-VAE maintains strong generation quality across diffusion
                models with and without REPA (e.g., gFID <b>3.46</b> on SiT-XL w/ REPA and <b>4.20</b> on DiT-XL w REPA)
            </li>
        </ul>

        <center>
            <div class="figure">
                <img src="assets/e2e-vae-eval.webp" alt="Drop-in VAE Performance Comparison"
                    style="max-width: 100%; height: auto;" loading="lazy">
            </div>
        </center>
    </section>

    <section id="4-from-scratch-training">
        <h2>4. Enables Effective From-Scratch Training</h2>
        <hr>

        <div style="display: flex; align-items: top; gap: 40px;">
            <div style="flex: 1;">
                <ul>
                    <li><b>End-to-end training from scratch:</b> REPA-E can jointly train both VAE and LDM from scratch
                        in an end-to-end manner, without requiring VAE pre-training</li>
                    <li><b>Strong performance even without initialization:</b> While initializing the VAE with
                        pretrained weights helps slightly improve results, from-scratch training still achieves gFID of
                        <b>4.34</b> at 80 epochs, significantly outperforming REPA (<b>7.90</b>)
                    </li>
                </ul>
            </div>

            <div style="flex: 1; margin-top: 8px;">
                <div class="figure">
                    <img src="assets/repae-scratch-eval.webp" alt="From-Scratch Training Performance"
                        style="width: 100%;" loading="lazy">
                </div>
            </div>
        </div>
    </section>

    <section id="bibtex">
        <h2>Citation</h2>
        <hr>
        <pre><code>@article{leng2025repae,
  title={REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion Transformers},
  author={Xingjian Leng and Jaskirat Singh and Yunzhong Hou and Zhenchang Xing and Saining Xie and Liang Zheng},
  year={2025},
  journal={arXiv preprint arXiv:2504.10483},
}</code></pre>
    </section>

    <footer
        style="margin-top: 40px; padding-top: 20px; border-top: 1px solid #ddd; font-size: 12px; color: #888; text-align: center;">
        <p>We thank the <a href="https://github.com/sihyun-yu/REPA/tree/gh-pages" target="_blank">REPA project</a> for
            the website template. Last updated: April 2025.</p>
    </footer>

    </div>
</body>

</html>